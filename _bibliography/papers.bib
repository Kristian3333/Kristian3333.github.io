---
---

@string{aps = {American Physical Society,}}


@article{barman2021ibe,
  abbr={EJPS},
  bibtex_show={true},
  title={IBE in engineering science-the case of malfunction explanation},
  author={Barman, Kristian Gonz{\'a}lez and van Eck, Dingmar},
  journal={European Journal for Philosophy of Science},
  volume={11},
  number={1},
  pages={1--19},
  year={2021},
  publisher={Springer},
  abstract={In this paper we investigate how inference to the best explanation (IBE) works in engineering science, focussing on the context of malfunction explanation. While IBE has gotten a lot of attention in the philosophy of science literature, few, if any, philosophical work has focussed on IBE in engineering science practice. We first show that IBE in engineering science has a similar structure as IBE in other scientific domains in the sense that in both settings IBE hinges on the weighing of explanatory virtues. We then proceed to show that, due to the intimate connection between explanation and redesign in engineering science, there is a further engineering domain-specific virtue in terms of which engineering malfunction explanations are evaluated, viz. the virtue of redesign utility. This virtue entails that the explanatory information offered by a malfunction explanation should be instrumental in predicting counterfactual dependencies in redesigned systems. We illustrate and elaborate these points in terms of a number of engineering examples, focussing in particular on the 2009 crash of Air France Flight 447. Our extension of analyses of IBE and explanation to engineering science practice offers new insights by identifying a new explanatory virtue in malfunction explanation: redesign utility.},
  html={https://link.springer.com/article/10.1007/s13194-020-00325-6},
  }

@article{weber2021quantum,
  abbr={FOC},
  bibtex_show={true},
  title={Quantum mechanical atom models, legitimate explanations and mechanisms},
  author={Weber, Erik and Lefevere, Merel and Barman, Kristian Gonzalez},
  journal={Foundations of Chemistry},
  volume={23},
  number={3},
  pages={407--429},
  year={2021},
  publisher={Springer},
  abstract={The periodic table is one of the best-known systems of classification in science. Because of the information it contains, it raises explanation-seeking questions. Quantum mechanical models of the behaviour of electrons (which describe an electron configuration for each kind of atom) may be seen as providing explanations in response to these questions. In this paper we first address the question ‘Do quantum mechanical models of atoms provide legitimate explanations?’ Because our answer is positive, our next question is ‘Are the explanations provided by quantum mechanical models of atoms mechanistic explanations?’. This question is motivated by the fact that in many scientific disciplines, mechanistic explanations are abundant. Because our answer to the second question is negative, our last question is ‘What kind of explanation do quantum mechanical models of atom provide?’ By addressing these questions, we shed light on the nature of an important type of chemical explanation.},
  html={https://link.springer.com/article/10.1007/s10698-021-09408-5}
}

@article{barman2022fictional,
  abbr={EJPS},
  bibtex_show={true},
  title={Fictional Mechanism Explanations: Clarifying Explanatory Holes in Engineering Science},
  author={Barman, Kristian Gonz{\'a}lez},
  journal={European Journal for Philosophy of Science},
  volume={12},
  number={2},
  pages={1--19},
  year={2022},
  publisher={Springer},
  abstract={This paper discusses a class of mechanistic explanations employed in engineering science where the activities and organization of nonstandard entities (voids, cracks, pits…) are cited as core factors responsible for failures. Given the use of mechanistic language by engineers and the manifestly mechanistic structure of these explanations, I consider several interpretations of these explanations within the new mechanical framework (among others: voids should be considered as shorthand expressions for other entities, voids should be reduced to lower-level mechanisms, or the explanations are simply abstract mechanistic explanations). I argue that these interpretations fail to solve several philosophical problems and propose an account of fictional mechanism explanations instead. According to this account, fictional mechanism explanations provide descriptions of fictional mechanisms that enable the tracking of counterfactual dependencies of the physical system they model by capturing system constraints. Engineers use these models to learn about and understand properties of materials, to build computational simulations of their behaviour, and to design new materials.},
  html={https://link.springer.com/article/10.1007/s13194-022-00468-8}
}

@article{weber2022thinking,
  abbr={JTSB},
  bibtex_show={true},
  title={Thinking about laws in political science (and beyond)},
  author={Weber, Erik and Makhnev, Karina and Leuridan, Bert and Gonzalez Barman, Kristian and De Coninck, Thijs},
  journal={Journal for the Theory of Social Behaviour},
  volume={52},
  number={1},
  pages={199--222},
  year={2022},
  publisher={Wiley Online Library},
  abstract={There are several theses in political science that are usually explicitly called ‘laws’. Other theses are generally thought of as laws, but often without being explicitly labelled as such. Still other claims are well-supported and arguably interesting, while no one would be tempted to call them laws. This situation raises philosophical questions: which theses deserve to be called laws and which not? And how should we decide about this? In this paper we develop and motivate a strategy for thinking about laws in political science which integrates two core concepts: spatio-temporal stability and social mechanisms. The proposed strategy is a set of guidelines that political scientists can use to reflect on and argue about specific cases within their discipline, not a clear-cut demarcation criterion. We defend and motivate this strategy and apply it to two cases (one with respect to state repression, one about parliamentary elections). After we have developed and motivated our strategy for political science, we show that our proposal is relevant for other disciplines in the social sciences. We explain how our views fit into critical realism and embed them in the debate on laws in general philosophy of science and in the philosophy of the social sciences.},
  html={https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsb.12313}
}



@article{barman2022procedure,
  abbr={AIEDAM},
  bibtex_show={true},
  title={Procedure for assessing the quality of explanations in failure analysis},
  author={Barman, Kristian Gonz{\'a}lez},
  journal={Artificial Intelligence for Engineering Design and Manufacturing},
  volume={36},
  year={2022},
  publisher={Cambridge University Press},
  abstract={This paper outlines a procedure for assessing the quality of failure explanations in engineering failure analysis. The procedure structures the information contained in explanations such that it enables to find weak points, to compare competing explanations, and to provide redesign recommendations. These features make the procedure a good asset for critical reflection on some areas of the engineering practice of failure analysis and redesign. The procedure structures relevant information contained in an explanation by means of structural equations so as to make the relations between key elements more salient. Once structured, the information is examined on its potential to track counterfactual dependencies by offering answers to relevant what-if-things-had-been-different questions. This criterion for explanatory goodness derives from the philosophy of science literature on scientific explanation. The procedure is illustrated by applying it to two case studies, one on Failure Analysis in Mechanical Engineering (a broken vehicle shaft) and one on Failure Analysis in Civil Engineering (a collapse in a convention center). The procedure offers failure analysts a practical tool for critical reflection on some areas of their practice while offering a deeper understanding of the workings of failure analysis (framing it as an explanatory practice). It, therefore, allows to improve certain aspects of the explanatory practices of failure analysis and redesign, but it also offers a theoretical perspective that can clarify important features of these practices. Given the programmatic nature of the procedure and its object (assessing and refining explanations), it extends work on the domain of computational argumentation.},
  html={https://www.cambridge.org/core/journals/ai-edam/article/abs/procedure-for-assessing-the-quality-of-explanations-in-failure-analysis/4ACAEC331448137A5B79BC55C2D05A83},
}

@article{gonzalez2023accident,
  abbr={ES},
  title={Accident Causation Models: The Good the Bad and the Ugly},
  author={Gonz{\'a}lez Barman, Kristian},
  journal={Engineering Studies},
  pages={1--26},
  year={2023},
  publisher={Taylor \& Francis},
  abstract={The main aim of this paper is to evaluate the evolution of Accident Causation Models (ACMs) from the perspective of philosophy of science. I use insights from philosophy of science to provide an epistemological analysis of the ways in which engineering scientists judge the value of different types of ACMs and to offer normative reflection on these judgements. I review three widespread ACMs and clarify their epistemic value: sequential models, epidemiological models, and systemic models. I first consider how they produce and ensure safety (‘usefulness’) relative to each other. This is evaluated in terms of the ability of models to afford a larger set of relevant counterfactual inferences. I take relevant inferences to be ones that provide safety (re)design information or suggest countermeasures (safety-design-interventions). I argue that systemic models are superior at providing said safety information. They achieve this, in part, by representing non-linear causal relationships. The second issue is whether we should retire linear and epidemiological models. I argue negatively. If the goal is to assign blame, linear models are better candidates. The reason is that they can provide semantic simplicity. Similarly, epidemiological models are better suited for the goal of audience communication because they can provide cognitive salience.},
  html={https://www.tandfonline.com/doi/full/10.1080/19378629.2023.2205024},
}

@article{barman2023towards,
  abbr={M&M},
  title={Towards a Benchmark for Scientific Understanding in Humans and Machines},
  author={Barman, Kristian Gonzalez and Caron, Sascha and Claassen, Tom and de Regt, Henk},
  journal={Minds and Machines},
  year={2023},
  abstract={Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by a set of these tests, allows for the evaluation and comparison of different approaches. Benchmarking plays a crucial role in establishing trust, ensuring quality control, and providing a basis for performance evaluation. By aligning machine and human scientific understanding we can improve their utility, ultimately advancing scientific understanding and helping to discover new insights within machines.},
  html={https://link.springer.com/article/10.1007/s11023-024-09657-1},
  selected={true}
}

@article{barman2023exploring,
  abbr={ZFN},
  title={Exploring the epistemic and ontic conceptions of Models and Idealizations in Science},
  author={Barman, Kristian Gonzalez},
  journal={Philosophical Problems in Science (Zagadnienia Filozoficzne w Nauce)},
  number={74},
  pages={295--301},
  year={2023}
}

@article{weber2024distinctively,
  abbr={Syn},
  title={Distinctively generic explanations of physical facts},
  author={Weber, Erik and Barman, Kristian Gonz{\'a}lez and De Coninck, Thijs},
  journal={Synthese},
  volume={203},
  number={4},
  pages={102},
  year={2024},
  html={https://link.springer.com/article/10.1007/s11229-024-04486-2},
  abstract={We argue that two well-known examples (strawberry distribution and Konigsberg bridges) generally considered genuine cases of distinctively mathematical explanation can also be understood as cases of distinctively generic explanation. The latter answer resemblance questions (e.g., why did neither person A nor B manage to cross all bridges) by appealing to ‘generic task laws’ instead of mathematical necessity (as is done in distinctively mathematical explanations). We submit that distinctively generic explanations derive their explanatory force from their role in ontological unification. Additionally, we argue that distinctively generic explanations are better seen as standardly mathematical instead of distinctively mathematical. Finally, we compare and contrast our proposal with the work of Christopher Pincock on abstract explanations in science and the views of Michael Strevens on abstract causal event explanations.},
  publisher={Springer}
}

@article{barman2024beyond,
  abbr={ETIN},
  title={Beyond transparency and explainability: on the need for adequate and contextualized user guidelines for LLM use},
  author={Barman, Kristian Gonz{\'a}lez and Wood, Nathan and Pawlowski, Pawel},
  journal={Ethics and Information Technology},
  volume={26},
  number={3},
  pages={47},
  year={2024},
  html={https://link.springer.com/article/10.1007/s10676-024-09778-2},
  abstract={Large language models (LLMs) such as ChatGPT present immense opportunities, but without proper training for users (and potentially oversight), they carry risks of misuse as well. We argue that current approaches focusing predominantly on transparency and explainability fall short in addressing the diverse needs and concerns of various user groups. We highlight the limitations of existing methodologies and propose a framework anchored on user-centric guidelines. In particular, we argue that LLM users should be given guidelines on what tasks LLMs can do well and which they cannot, which tasks require further guidance or refinement by the user, and context-specific heuristics. We further argue that (some) users should be taught to refine and elaborate adequate prompts, be provided with good procedures for prompt iteration, and be taught efficient ways to verify outputs. We suggest that for users, shifting away from looking at the technology itself, but rather looking at the usage of it within contextualized sociotechnical systems, can help solve many issues related to LLMs. We further emphasize the role of real-world case studies in shaping these guidelines, ensuring they are grounded in practical, applicable strategies. Like any technology, risks of misuse can be managed through education, regulation, and responsible development.},
  publisher={Springer Netherlands Dordrecht},
  selected={true}
}

@article{gonzalez2025reinforcement,
  abbr={P&T},
  title={Reinforcement learning from human feedback in LLMs: Whose culture, whose values, whose perspectives?},
  author={Gonz{\'a}lez Barman, Kristian and Lohse, Simon and de Regt, Henk W},
  journal={Philosophy \& Technology},
  volume={38},
  number={2},
  pages={1--26},
  year={2025},
  html={https://link.springer.com/article/10.1007/s13347-025-00861-0},
  abstract={We argue for the epistemic and ethical advantages of pluralism in Reinforcement Learning from Human Feedback (RLHF) in the context of Large Language Models (LLMs). Drawing on social epistemology and pluralist philosophy of science, we suggest ways in which RHLF can be made more responsive to human needs and how we can address challenges along the way. The paper concludes with an agenda for change, i.e. concrete, actionable steps to improve LLM development. }
  publisher={Springer}
}

@article{van2024prospects,
  abbr={Chapter},
  title={Technologically Driven Causal Inference: Prospects and Challenges},
  author={van Eck, Dingmar and Barman, Kristian Gonz{\'a}lez},
  journal={The Routledge Handbook of Causality and Causal Methods},
  pages={342},
  year={2024},
  publisher={Taylor \& Francis},
  html={https://www.taylorfrancis.com/books/edit/10.4324/9781003528937/routledge-handbook-causality-causal-methods-phyllis-illari-federica-russo}
}
@article{pawlowski2025fortifying,
  abbr={P&T},
  title={Fortifying Trust: Can Computational Reliabilism Overcome Adversarial Attacks?},
  author={Pawlowski, Pawel and Barman, Kristian Gonz{\'a}lez},
  journal={Philosophy \& Technology},
  volume={38},
  number={1},
  pages={21},
  year={2025},
  publisher={Springer},
  html={https://link.springer.com/article/10.1007/s13347-025-00851-2},
  abstract={Computational Reliabilism (CR) has emerged as a promising framework for assessing the trustworthiness of AI systems, particularly in domains where complete transparency is infeasible. However, the rise of sophisticated adversarial attacks poses a significant challenge to CR’s key reliability indicators. This paper critically examines the robustness of CR in the face of evolving adversarial threats, revealing the limitations of verification and validation methods, robustness analysis, implementation history, and expert knowledge when confronted with malicious actors. Our analysis suggests that CR, in its current form, is inadequate to address the dynamic nature of adversarial attacks. We argue that while CR’s core principles remain valuable, the framework must be extended to incorporate adversarial resilience, adaptive reliability criteria, and context-specific reliability thresholds. By embracing these modifications, CR can evolve to provide a more comprehensive and resilient approach to assessing AI reliability in an increasingly adversarial landscape.}
}
@article{barman2025large,
  abbr={arXiv},
  title={Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models},
  author={Barman, Kristian G and Caron, Sascha and Sullivan, Emily and de Regt, Henk W and de Austri, Roberto Ruiz and Boon, Mieke and F{\"a}rber, Michael and Fr{\"o}se, Stefan and Hasibi, Faegheh and Ipp, Andreas and others},
  journal={arXiv preprint arXiv:2501.05382},
  year={2025},
  html={https://arxiv.org/abs/2501.05382},
  abstract={This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we call Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data - are tailored to address the demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability by testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models.},
  selected={true}
}
@article{barman2025IBE,
  abbr={Chapter},
  title={(forthcoming) Inference to the Best Explanation in Explainable AI},
  author={Barman, Kristian G.},
  journal={Philosophy of Artificial Intelligence},
  year={2025},
  abstract={This paper investigates the role of Inference to the Best Explanation (IBE) in Explainable Artificial Intelligence (XAI), arguing that IBE can both act as a framework for revealing the considerations and preferences behind XAI explanations and for guiding and for evaluating them in a more rigorous manner. The paper focusses specifically on ad hoc explainability and model agnostic techniques, illustrating the argument through examples of salience maps and feature importance techniques. Here, IBE in XAI happens in two steps. The first aims at maximizing the range and accuracy of possible inferences made; the second aims to provide a certain balance of explanatory virtues given a certain why-question and stakeholder that requires an explanation. Using the language of explanatory virtues enables a more nuanced discussion about what we may like or dislike about certain explanations in the current XAI landscape and provides a straightforward way of improving explanations.} 
}
@article{barman2025AI,
  abbr={GJPS},
  title={(forthcoming) Is Automated Discovery Expanding Human Understanding? On the role of Machine Learning in the increase of scientific understanding.},
  author={Barman, Kristian G.},
  journal={General Journal for Philosophy of Science},
  year={2025},
  abstract={Automated discovery techniques, driven by computational methods, have played an important role in achieving scientific breakthroughs across various disciplines. This paper investigates the impact of automated discovery on scientific understanding through the examination of two case studies: quantum optics experiment design and accelerated materials discovery of vanadium selenites. I argue that these systems enrich researchers' understanding by revealing novel phenomena and uncovering solutions that lie beyond the confines of manual search, yet nonetheless once discovered can be integrated into pre-existing theoretical scaffolding. I further study the role played by machine learning models involved in automated discovery as it relates to the increase scientific understanding, arguing that they can be taken to be INUS conditions. Recognizing the capacity of automated discovery to improve understanding, the paper submits that the logic of discovery remains crucially relevant and warrants further in-depth conceptual development.}
}
@article{barman2025DME,
  abbr={BJPS},
  title={(forthcoming) Distinctively Mathematical Explanations of Game Outcomes},
  author={Barman, Kristian G.},
  journal={British Journal for the Philosophy of Science},
  year={2025},
  html={https://www.journals.uchicago.edu/doi/10.1086/735281},
  abstract={The debate on distinctively mathematical explanations (DMEs) relies heavily on examples such as strawberry distribution and bridge crossing.  I argue that "task-based" examples do not explain physical facts; rather, they explain outcomes of abstract games that follow predefined rules. The impossibility of these tasks stems from obeying the games’ rules. Modality holds due to the way the explanandum is formulated, rather than due to nomological or mathematical constraints. I develop an account of these examples as mathematical explanations of game outcomes rather than DMEs of physical facts, where mathematical models enable unification and counterfactual reasoning about game outcomes across instantiations, providing explanatory power. I then show how this account can provide insights on the range of valid counterfactual inferences enabled by these explanations.}
}
@article{barman2025Cau,
  abbr={JME},
  title={(forthcoming) Reframing the Responsibility Gap in Medical AI: Insights from Causal Selection and Authorship Attribution},
  author={Barman, Kristian G.},
  journal={Journal of Medical Ethics},
  year={2025},
  abstract={The increasing use of artificial intelligence (AI) in healthcare has sparked debates about responsibility and accountability for AI-related errors. The difficulty in attributing moral responsibility for undesirable outcomes caused by increasingly autonomous (often opaque) AI systems has become a new focal point in the debate on ‘responsibility gaps’. We approach the problem of these gaps by offering a framework that combines causal selection principles from the philosophy of science with recent accounts of authorship attribution in AI contexts. We argue this framework offers a more comprehensive and context-sensitive approach to the responsibility gap in medical AI.}
}